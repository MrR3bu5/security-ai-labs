# PentestGPT in a Segmented Homelab  
## Research Evaluation of Autonomous AI Penetration Concepts from a Defensive Security Perspective

---

## Overview

This write-up documents the deployment and evaluation of **PentestGPT** inside a controlled cybersecurity homelab.  
The goal was not to automate penetration testing, but to understand how emerging AI-driven offensive tooling behaves when introduced into a defensive, detection-focused environment.

The evaluation focuses on architecture, risk awareness, and alignment with ethical security engineering practices.

---

## Objectives

The project explored three primary questions:

- Can an AI-driven penetration framework be deployed safely inside a segmented lab?
- Does the tool behave as an assistant, or does it encourage autonomous execution?
- How does this class of tooling fit into a Purple Team workflow centered on detection engineering?

The evaluation was conducted from a defensive security perspective rather than an offensive automation perspective.

---

## Lab Architecture Context

PentestGPT was deployed on a dedicated analysis VM within an existing segmented homelab environment.

### Network Placement

- **AI Analysis Node (ai-sec-01)**  
  Ubuntu 22.04 VM
- Located on the **LAN / Admin network**
- Accessed through a jump host
- No direct trust relationship with:
  - Domain Controller
  Client VLAN
  Attack VLAN

### Infrastructure Stack

- Proxmox virtualization platform
- OPNsense firewall with VLAN segmentation
- Wazuh SIEM for telemetry collection
- Docker containerization for AI tooling

The assistant VM was intentionally isolated to prevent uncontrolled interaction with sensitive lab systems.

---

## Deployment Summary

PentestGPT was deployed using Docker to maintain isolation and reproducibility.

Key deployment observations:

- Python 3.12 requirement prevented direct installation on Ubuntu 22.04
- Container resource limits required modification to match available CPU resources
- Service API keys were used instead of personal credentials
- No external services were exposed beyond outbound HTTPS

The containerized approach allowed rapid testing without modifying base system dependencies.

---

## Observed Tool Behavior

PentestGPT presents itself as an autonomous penetration testing workflow driven by large language models.

During evaluation:

- The tool requires a target context at launch
- Interaction flow encourages reconnaissance and exploitation planning
- Non-interactive modes suggest automated execution workflows
- Interactive mode still frames sessions around attack progression

These behaviors indicate that the project is designed primarily for automation experimentation rather than advisory assistance.

---

## Security Engineering Assessment

From a defensive engineering standpoint, several observations emerged.

### Autonomy vs Assistance

The tool's design leans toward operational automation rather than human-guided analysis.  
For a Purple Team workflow focused on telemetry and detection, this introduces risk:

- Reduced human oversight
- Potential overreach beyond intended testing scope
- Increased need for strict network controls

### Ethical Boundaries

The evaluation highlighted the importance of maintaining a clear boundary:

- AI can assist reasoning
- AI should not independently execute attack workflows

Maintaining a **human-in-the-loop model** remains essential.

### Architecture Implications

Running the tool inside a segmented analysis network proved effective:

- Prevented unintended lateral movement
- Allowed controlled observation of behavior
- Enabled research without exposing production-like systems

This reinforces the value of network segmentation when experimenting with AI security tooling.

---

## Lessons Learned

Key technical and architectural lessons:

- Many AI security tools assume larger infrastructure resources than typical homelabs
- Container isolation is critical when testing automation-focused software
- Service-based API key usage provides better operational hygiene
- Marketing language around autonomy should be evaluated carefully against real behavior

---

## Decision Outcome

PentestGPT will remain in the lab as a **research artifact** rather than an operational tool.

Primary reasoning:

- The autonomous workflow model does not align with a detection-driven Purple Team approach
- Human-guided AI assistants better match ethical security engineering goals

Future work will focus on AI systems that support:

- analysis
- documentation
- detection hypothesis development

without autonomous execution.

---

## Value to the Security-AI-Lab Project

This evaluation demonstrates:

- Practical integration of AI tooling into a segmented security architecture
- Critical assessment of emerging AI security trends
- Responsible experimentation within ethical boundaries

Rather than adopting automation blindly, the project emphasizes engineering judgment and defensive awareness.

---

## Future Exploration

Potential follow-on research areas:

- AI-assisted detection engineering workflows
- Integration of assistant-style LLM tooling with SIEM telemetry
- Secure prompt handling and auditability within analysis environments

---

## Closing Thoughts

AI-driven security tooling will continue to evolve.  
The responsibility of the security engineer is not only to build and deploy tools, but to evaluate their behavior, risks, and alignment with real-world defensive practices.

This research entry represents that evaluation process.

